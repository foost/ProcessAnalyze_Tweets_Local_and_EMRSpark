# Process and Analyze Tweets Laptop and with EMR Spark

This repository analize Tweets from two perspectives, with a local machine and a sample dataset, and with AWS EMR Spark with the complete original dataset, based on each user necessities.

## Laptop

The laptop option, focus on explore a stored dataset with only the geotagged Tweets. By apppliying some regex methods, SQLqueriying, NLP techniques, plotting and mapping. This option is for newbies that want to explore a stored Twitter dataset.

The file SampleLaptopScenario.py do the following:

* connect and extract tweets from a PostgreSQL database
* query the database extracting only the geotagged tweets
* remove users with more than 3 identical tweets and with more than 3000 tweets
* remove non alphanumeric characters
* detect the language between Dutch and English
* tokenize and remove words from a stop words list
* apply a stemmer to the tweets
* group and count all the words in the data set (frequency)
* count all hashtags in English and Dutch
* count most frequent bigrams in English and Dutch
* Deliver a co-occurance matrix within specific element in an array
* create two charts with most frequent terms in Dutch and the count of elements in dutch and english, also provide a HTML file with an interactive visualization of the geotagged tweets per day.

The instructions to change variables are inside the instructions.txt

## Amazon EMR

The file Create_EMR_AWScli.sh contains the instructions to configure CLI and to install a cluster in EMR with Spark. Also to sent a pyspark script file that contains a script that process raw Twitter data the variables that have to be adapted to be replicated. The cloud EMR spark gives the instructions to start a cluster in Amazon CLI with ten m4.4xlarge instances (requires to have permission from AMS to request ten instances) and to run it in cluster mode. Also contain instructuions to configure the Amazon CLI in an Ubuntu OS 18.04.

The file TestCloudSparkEMR_firefox_geocode.py do the following:

* read all the records as JSON objects, 
* remove special characters (non alphanumeric),
* remove specific elements of a tweet (URLâ€™s, retweets),
* remove duplicated Tweets,
* load a gazetteer with townships and cities of the Netherlands dropping duplicates, 
* tokenize all the elements, 
* remove the stop words in Dutch and English, 
* group and count all the words in the data set (frequency),
9)extract all unigrams and match them with the gazetteer (geocoding), 
10)deliver two files with the most frequent words (JSON) and the geocoded tweets with latitude and longitude coordinates (CSV), and the necessary data to create the co-occurrence matrix (JSON).

The instructions to change variables are inside the instructions.txt
